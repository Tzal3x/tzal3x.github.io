<!doctype html>
<html>
    <head>
        <title>Alex Tzimas | Personal Website</title>
        <link
            rel="stylesheet"
            href="https://unpkg.com/terminal.css@0.7.4/dist/terminal.min.css"
        />
        <link rel="icon" href="favicon.svg" />
        <link rel="stylesheet" href="style.css">
        <meta charset="UTF-8" />
        <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    </head>
    <script>hljs.initHighlightingOnLoad();</script>
    <script>
        function sendEmail() {
            window.location = "mailto:tzalex@outlook.com.gr";
        }
    </script>
    <body>
        <div
            class="root"
            style="
                display: flex;
                justify-content: flex-start;
                flex-direction: column;
                align-items: center;
                height: 100vh;
                padding: 20px;
            "
        >
            <header>
                <h1 class="terminal-prompt">Alex Tzimas</h1>
            </header>
            <div style="width: 100%; max-width: 800px">
                <nav>
                    <ul
                        style="
                            display: flex;
                            flex-direction: row;
                            justify-content: space-evenly;
                            gap: 24px;
                            flex-wrap: wrap;
                        "
                    >
                        <!-- TODO - include CV too! -->
                        <li><a href="./index.html">Home</a></li>

                        <li><a href="./about.html">About Me</a></li>

                        <!-- TODO -->
                        <li>
                            <a href="./resources.html"
                                >Resources & Good reads</a
                            >
                        </li>
                    </ul>
                </nav>
            </div>
            <div
                style="70ch;
                width: 100%;
                max-width: 800px;
                margin-top: 20px;
                padding: 20px;
              line-height: 1.75;"
            >
                <h2 style="text-align: center">
                    Cutting <span style="color: red;">$350K</span>/year in hosting costs
                </h2>
                <h3>
                    TL;DR
                </h3>
                <p>
                   This is a short story about how we managed to reduce the estimated hosting costs of a Kubernetes cluster,
                   just before going live in production. In short, we had to resolve a memory leak issue and migrate
                   the HTTP server framework (from <a href="https://nextjs.org/docs/pages/building-your-application/routing/api-routes">Next.js routes</a> to a <a href="https://bun.com/docs/api/http">Bun server</a>).
                </p>
                <h3>
                   About the project
                </h3>
                <p>
                    This story takes place during the <a href="https://academy.binance.com/en/glossary/mainnet">mainnet-launch</a> period of
                    <a href="https://www.walrus.xyz">Walrus</a> and <a href="https://github.com/mystenLabs/walrus-sites">Walrus Sites</a>,
                    in early March 2025.
                    <ul>
                        <li>Walrus is a decentralized storage solution. Think of it like Google Cloud Storage or Amazon S3, but "shards" of your files
                        are distributed across multiple servers, competing with <a href="http://arweave.org">Arweave</a> and <a href="http://ipfs.tech">IPFS</a>.</li>
                        <br>
                        <li>Walrus Sites is a website hosting solution whose main principle is that file resources (JS, CSS, HTML)
                        are saved on Walrus.</li>
                    </ul>
                    These websites are accessed through "portals" that translate user requests into HTTP calls to Walrus.
                    For example, <code>https://wal.app</code> is the domain of the server portal. If you need to access your site,
                    you would request <code>https://name-of-your-site.wal.app</code>. This way, the <code>wal.app</code> portal will
                    fetch the <code>name-of-your-site</code> resources and serve them to the end user.
                    <br>
                    My colleagues and I were responsible for building the core features of the product, as well as optimizing both speed
                    (increasing requests per second) and reducing cost.
                    Let's explore how things turned out.
                </p>
                <h3>
                    Ready, set... wait! Can we even launch?
                </h3>
                <p>
                    At that time, we were three weeks away from launching on mainnet. This would mean
                    an exponential increase in traffic.
                    <br>
                    We thought we were ready until the frontend of a famous NFT airdrop was hosted on our (at the time) live Walrus Sites testnet (staging) portal. Airdrops in general are infamous for
                    creating traffic spikes, and this was no exception. Our portal was flooded with requests,
                    until it became unresponsive. Users were not very happy, nor were stakeholders.
                    <br>
                </p>
                <h3>
                    The two culprits
                </h3>
                <p>
                    During the post-mortem, we took a look at the charts inside our Kubernetes cluster.
                    The results were the following:
                </p>
                <ol>
                    <li>During the airdrop, which lasted one day, we received approximately 113K requests.
                        At the peak of the event, we had 180 req/sec, and things started looking bad at around 100 req/sec.
                        We had three Kubernetes pods then, which means 33 req/sec were enough to overload a single pod.
                    </li>
                    <br>
                    <li>Looking at the memory usage of each container, the amount of RAM used was only increasing despite the load,
                    	which is a bad sign for memory leaks.</li>
                </ol>
                <p>
                    After the incident, we made some adjustments and scaled out to 15 pods per region, which is 45 pods in total.
                    Each Kubernetes node could take about 12-16 pods, which meant we were able to handle 150 req/sec
                    in each region for the time being.
                    <br>
                    But this was not enough. When Walrus Sites became popular, those numbers were not enough to handle future -unforgiving-
                    airdrops. To provision the cluster to serve 1500 req/sec, we would need to spin up 30 Kubernetes nodes,
                    <b>costing $30k per month</b>.
                    <br>
                    Evidently, we needed to move fast and fix things.
                </p>
                <h3>
                    Fixing the memory leak
                </h3>
                <p>
                    To fix a memory leak, you first need to understand what it is.
                    By definition, memory leaks occur when a program allocates memory but fails to release it after use.
                    Most of you might know this term from C or C++, since those languages don't have a <a href="https://en.wikipedia.org/wiki/Garbage_collection_(computer_science)">garbage collector (GC)</a>.
                    But in our case, we were using a JavaScript runtime, which does implement it.
                    So why was it possible to have a memory leak here?
                    <br>
                    The fact that high-level languages have GC is a frequent source of confusion: it can give developers the false impression that
                    they don't need to worry about memory management.
                </p>

                <p>But they do. The reason is <em>lingering references</em>. Let me explain:</p>

                <p>
                The most common GC algorithm is <a href="https://en.wikipedia.org/wiki/Tracing_garbage_collection#Na%C3%AFve_mark-and-sweep">Mark-and-Sweep</a>.
                In essence, at the start of a program, a <em>root-set</em> is created which contains references to root objects.
                Root objects are starting points of the program such as global variables, stack variables — anything the program
                can directly reach without following pointers. Given this, the algorithm is very simple:
                <ol>
                <li>When an object is created, its parent references it and it’s marked as in use.</li>
                <li>Deleting an object removes its reference from the parent, marking it as unused.</li>
                <li>Periodically, the GC traverses reference trees, deallocating unreachable (unmarked) objects.</li>
                </ol>
                Therefore, to have a memory leak, we need to always have a reference pointed to an object.
                The cases where this holds true are:
                <ol>
                <li>Circular references: an object A points to an object B, and at the same time B points to A. This means both A and B always have a reference
                pointed at them, marking them as "in use" by the GC.</li>
                <li>Adding objects to root objects without removing them, like a list that keeps growing.</li>
                </ol>
                In our case, the issue lay in the latter. At that time, we were tracking requests using Sentry.
                Because Sentry by default didn't provide the details that we needed, we had to add custom tags to each log message.
<pre><code>import logger from "@lib/logger";
import * as Sentry from "@sentry/bun";

function addLoggingArgsToSentry(args: { [key: string]: any }) {
    Object.entries(args).forEach(([key, value]) => {
        if (key !== "message") {
        	Sentry.setTag(key, value); // memleak🚰
        }
    });
}</code></pre>
				<p>
				The dependency Sentry object was a global. Calling <code>Sentry.setTag(key, value)</code> increased the Sentry object size upon each
				request, resulting in a memory leak! Migrating our logs to Grafana solved this issue. Easy peasy.
				</p>
				<h3>
					Improving the HTTP server's performance
				</h3>
				<p>
					Fixing the memory leak was good, but we could do better.
					While Next.js is convenient for quickly bootstrapping and deploying proof of concepts through excellent integration with Vercel,
					we had to consider the following:
					<br>
					While our HTTP server included just a few lines of code:
				</p>
				<ol>
					<li>Next.js comes with its own “batteries included” handlers in /_next, plus a <a href="https://security.snyk.io/package/npm/next">long security baggage</a>.</li>
					<li>Awkward routing: we had to find "hacky" ways to override the file-based routing logic when handling subdomains like <code>https://docs.wal.app</code>.</li>
					<li>Vercel did not support <a href="https://bun.sh">Bun</a> at the time, an alternative to Node.js with extraordinary performance.</li>
					<li>A lot of <a href="https://security.snyk.io/package/npm/next">security issues</a>. Remember <b>CVE-2025-29927</b>?</li>
				</ol>
				<p>
					So migrating to Bun as a package manager, runtime, and HTTP server improved both our performance and the readability of our code.
					The migration was simple, because Bun supports most Node.js APIs and package managers.
				</p>
				<h3>
					Results
				</h3>
				<p>
					Following these updates, we noticed the following improvements:
				</p>
				<ol>
					<li>During the first 12 hours after mainnet launch, we received 845.31k hits.</li>
					<li>CPU usage stable at ~2%, while memory usage stabilized at ~16-18%.</li>
					<li>Our backend servers could now handle 3000 req/sec (compared to 1500 req/sec that we would handle initially on prod data).</li>
					<li>Average response time dropped to 0.5 sec, with p99 being under 1 second.</li>
					<li><b>Costs</b>: For each pod, using 1 core CPU & 1G RAM, our node was 16 cores & 128G RAM.
						Using the c4-highram-16 compute engine pricing at $1.04/hr, we averaged around $760 a month,
						so 12 pods were about 12/16 * 760 = $570 a month (compared to $30,000 that we had initially). That is 12*($30,000-$570) = $353,160 in a year!</li>
					<li>We experienced no downtime whatsoever, even many months after launch!</li>
				</ol>
				<p>
					With these fixes in place, we were finally set to launch—no longer worried about NFT airdrop traffic spikes or dreaded hosting invoices!
				</p>
                <h3>
                    Why didn't we do this from the start?
                </h3>
                <p>
                	Because <a href="https://vercel.com/docs/functions">serverless</a>!
                 	Our initial setup was running on Vercel, which was expensive and infrastructure management was not under our own control.
                  	Because we wanted to be in complete control (and responsible) for how our servers scaled, we decided to migrate to a Kubernetes cluster.
                   	This meant that we migrated from a stateless environment (serverless) to a stateful (Kubernetes) one. i.e.:
                </p>
                <ul>
                	<li>Memory leaks were hidden because servers were spawning and restarting automatically, masking the RAM usage accumulation.</li>
                    <li>Next.js goes hand in hand with Vercel, using the <a href="https://vercel.com/docs/functions/runtimes/edge">edge runtime</a>,
	                    where the observed performance was sufficient for the initial steps of the project.</li>
                </ul>
                <p>
                    Overall, this was an interesting experience. Tackling challenges like these is what keeps our work exciting!
                </p>
            </div>
            <br/>
            <br/>
            <footer>
                <p>&copy; Alex Tzimas 2025</p>
            </footer>
        </div>
    </body>
</html>
